{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA (GPU support) is available in PyTorch!\n",
      "Number of CUDA devices available: 1\n",
      "CUDA Device 0: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU support) is available in PyTorch!\")\n",
    "    # Get the number of CUDA devices\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_gpus}\")\n",
    "    # Loop through the available devices and print their names\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA (GPU support) is not available in PyTorch. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saikr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saikr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saikr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from datasets import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sqlalchemy import MetaData, create_engine\n",
    "from sqlalchemy.exc import CompileError, NoReferencedColumnError\n",
    "from sqlalchemy.schema import CreateTable\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Databases:  [('academic', 'sqlite:///spider/database\\\\academic/academic.sqlite'), ('activity_1', 'sqlite:///spider/database\\\\activity_1/activity_1.sqlite'), ('aircraft', 'sqlite:///spider/database\\\\aircraft/aircraft.sqlite'), ('allergy_1', 'sqlite:///spider/database\\\\allergy_1/allergy_1.sqlite'), ('apartment_rentals', 'sqlite:///spider/database\\\\apartment_rentals/apartment_rentals.sqlite'), ('architecture', 'sqlite:///spider/database\\\\architecture/architecture.sqlite'), ('assets_maintenance', 'sqlite:///spider/database\\\\assets_maintenance/assets_maintenance.sqlite'), ('baseball_1', 'sqlite:///spider/database\\\\baseball_1/baseball_1.sqlite'), ('battle_death', 'sqlite:///spider/database\\\\battle_death/battle_death.sqlite'), ('behavior_monitoring', 'sqlite:///spider/database\\\\behavior_monitoring/behavior_monitoring.sqlite'), ('bike_1', 'sqlite:///spider/database\\\\bike_1/bike_1.sqlite'), ('body_builder', 'sqlite:///spider/database\\\\body_builder/body_builder.sqlite'), ('book_2', 'sqlite:///spider/database\\\\book_2/book_2.sqlite'), ('browser_web', 'sqlite:///spider/database\\\\browser_web/browser_web.sqlite'), ('candidate_poll', 'sqlite:///spider/database\\\\candidate_poll/candidate_poll.sqlite'), ('car_1', 'sqlite:///spider/database\\\\car_1/car_1.sqlite'), ('chinook_1', 'sqlite:///spider/database\\\\chinook_1/chinook_1.sqlite'), ('cinema', 'sqlite:///spider/database\\\\cinema/cinema.sqlite'), ('city_record', 'sqlite:///spider/database\\\\city_record/city_record.sqlite'), ('climbing', 'sqlite:///spider/database\\\\climbing/climbing.sqlite'), ('club_1', 'sqlite:///spider/database\\\\club_1/club_1.sqlite'), ('coffee_shop', 'sqlite:///spider/database\\\\coffee_shop/coffee_shop.sqlite'), ('college_1', 'sqlite:///spider/database\\\\college_1/college_1.sqlite'), ('college_2', 'sqlite:///spider/database\\\\college_2/college_2.sqlite'), ('college_3', 'sqlite:///spider/database\\\\college_3/college_3.sqlite'), ('company_1', 'sqlite:///spider/database\\\\company_1/company_1.sqlite'), ('company_employee', 'sqlite:///spider/database\\\\company_employee/company_employee.sqlite'), ('company_office', 'sqlite:///spider/database\\\\company_office/company_office.sqlite'), ('concert_singer', 'sqlite:///spider/database\\\\concert_singer/concert_singer.sqlite'), ('county_public_safety', 'sqlite:///spider/database\\\\county_public_safety/county_public_safety.sqlite'), ('course_teach', 'sqlite:///spider/database\\\\course_teach/course_teach.sqlite'), ('cre_Docs_and_Epenses', 'sqlite:///spider/database\\\\cre_Docs_and_Epenses/cre_Docs_and_Epenses.sqlite'), ('cre_Doc_Control_Systems', 'sqlite:///spider/database\\\\cre_Doc_Control_Systems/cre_Doc_Control_Systems.sqlite'), ('cre_Doc_Template_Mgt', 'sqlite:///spider/database\\\\cre_Doc_Template_Mgt/cre_Doc_Template_Mgt.sqlite'), ('cre_Doc_Tracking_DB', 'sqlite:///spider/database\\\\cre_Doc_Tracking_DB/cre_Doc_Tracking_DB.sqlite'), ('cre_Drama_Workshop_Groups', 'sqlite:///spider/database\\\\cre_Drama_Workshop_Groups/cre_Drama_Workshop_Groups.sqlite'), ('cre_Theme_park', 'sqlite:///spider/database\\\\cre_Theme_park/cre_Theme_park.sqlite'), ('csu_1', 'sqlite:///spider/database\\\\csu_1/csu_1.sqlite'), ('culture_company', 'sqlite:///spider/database\\\\culture_company/culture_company.sqlite'), ('customers_and_addresses', 'sqlite:///spider/database\\\\customers_and_addresses/customers_and_addresses.sqlite'), ('customers_and_invoices', 'sqlite:///spider/database\\\\customers_and_invoices/customers_and_invoices.sqlite'), ('customers_and_products_contacts', 'sqlite:///spider/database\\\\customers_and_products_contacts/customers_and_products_contacts.sqlite'), ('customers_campaigns_ecommerce', 'sqlite:///spider/database\\\\customers_campaigns_ecommerce/customers_campaigns_ecommerce.sqlite'), ('customers_card_transactions', 'sqlite:///spider/database\\\\customers_card_transactions/customers_card_transactions.sqlite'), ('customer_complaints', 'sqlite:///spider/database\\\\customer_complaints/customer_complaints.sqlite'), ('customer_deliveries', 'sqlite:///spider/database\\\\customer_deliveries/customer_deliveries.sqlite'), ('debate', 'sqlite:///spider/database\\\\debate/debate.sqlite'), ('decoration_competition', 'sqlite:///spider/database\\\\decoration_competition/decoration_competition.sqlite'), ('department_management', 'sqlite:///spider/database\\\\department_management/department_management.sqlite'), ('department_store', 'sqlite:///spider/database\\\\department_store/department_store.sqlite'), ('device', 'sqlite:///spider/database\\\\device/device.sqlite'), ('document_management', 'sqlite:///spider/database\\\\document_management/document_management.sqlite'), ('dog_kennels', 'sqlite:///spider/database\\\\dog_kennels/dog_kennels.sqlite'), ('dorm_1', 'sqlite:///spider/database\\\\dorm_1/dorm_1.sqlite'), ('driving_school', 'sqlite:///spider/database\\\\driving_school/driving_school.sqlite'), ('election', 'sqlite:///spider/database\\\\election/election.sqlite'), ('election_representative', 'sqlite:///spider/database\\\\election_representative/election_representative.sqlite'), ('employee_hire_evaluation', 'sqlite:///spider/database\\\\employee_hire_evaluation/employee_hire_evaluation.sqlite'), ('entertainment_awards', 'sqlite:///spider/database\\\\entertainment_awards/entertainment_awards.sqlite'), ('entrepreneur', 'sqlite:///spider/database\\\\entrepreneur/entrepreneur.sqlite'), ('epinions_1', 'sqlite:///spider/database\\\\epinions_1/epinions_1.sqlite'), ('e_government', 'sqlite:///spider/database\\\\e_government/e_government.sqlite'), ('e_learning', 'sqlite:///spider/database\\\\e_learning/e_learning.sqlite'), ('farm', 'sqlite:///spider/database\\\\farm/farm.sqlite'), ('film_rank', 'sqlite:///spider/database\\\\film_rank/film_rank.sqlite'), ('flight_1', 'sqlite:///spider/database\\\\flight_1/flight_1.sqlite'), ('flight_2', 'sqlite:///spider/database\\\\flight_2/flight_2.sqlite'), ('flight_4', 'sqlite:///spider/database\\\\flight_4/flight_4.sqlite'), ('flight_company', 'sqlite:///spider/database\\\\flight_company/flight_company.sqlite'), ('formula_1', 'sqlite:///spider/database\\\\formula_1/formula_1.sqlite'), ('game_1', 'sqlite:///spider/database\\\\game_1/game_1.sqlite'), ('game_injury', 'sqlite:///spider/database\\\\game_injury/game_injury.sqlite'), ('gas_company', 'sqlite:///spider/database\\\\gas_company/gas_company.sqlite'), ('geo', 'sqlite:///spider/database\\\\geo/geo.sqlite'), ('gymnast', 'sqlite:///spider/database\\\\gymnast/gymnast.sqlite'), ('hospital_1', 'sqlite:///spider/database\\\\hospital_1/hospital_1.sqlite'), ('hr_1', 'sqlite:///spider/database\\\\hr_1/hr_1.sqlite'), ('icfp_1', 'sqlite:///spider/database\\\\icfp_1/icfp_1.sqlite'), ('imdb', 'sqlite:///spider/database\\\\imdb/imdb.sqlite'), ('inn_1', 'sqlite:///spider/database\\\\inn_1/inn_1.sqlite'), ('insurance_and_eClaims', 'sqlite:///spider/database\\\\insurance_and_eClaims/insurance_and_eClaims.sqlite'), ('insurance_fnol', 'sqlite:///spider/database\\\\insurance_fnol/insurance_fnol.sqlite'), ('insurance_policies', 'sqlite:///spider/database\\\\insurance_policies/insurance_policies.sqlite'), ('journal_committee', 'sqlite:///spider/database\\\\journal_committee/journal_committee.sqlite'), ('loan_1', 'sqlite:///spider/database\\\\loan_1/loan_1.sqlite'), ('local_govt_and_lot', 'sqlite:///spider/database\\\\local_govt_and_lot/local_govt_and_lot.sqlite'), ('local_govt_in_alabama', 'sqlite:///spider/database\\\\local_govt_in_alabama/local_govt_in_alabama.sqlite'), ('local_govt_mdm', 'sqlite:///spider/database\\\\local_govt_mdm/local_govt_mdm.sqlite'), ('machine_repair', 'sqlite:///spider/database\\\\machine_repair/machine_repair.sqlite'), ('manufactory_1', 'sqlite:///spider/database\\\\manufactory_1/manufactory_1.sqlite'), ('manufacturer', 'sqlite:///spider/database\\\\manufacturer/manufacturer.sqlite'), ('match_season', 'sqlite:///spider/database\\\\match_season/match_season.sqlite'), ('medicine_enzyme_interaction', 'sqlite:///spider/database\\\\medicine_enzyme_interaction/medicine_enzyme_interaction.sqlite'), ('mountain_photos', 'sqlite:///spider/database\\\\mountain_photos/mountain_photos.sqlite'), ('movie_1', 'sqlite:///spider/database\\\\movie_1/movie_1.sqlite'), ('museum_visit', 'sqlite:///spider/database\\\\museum_visit/museum_visit.sqlite'), ('musical', 'sqlite:///spider/database\\\\musical/musical.sqlite'), ('music_1', 'sqlite:///spider/database\\\\music_1/music_1.sqlite'), ('music_2', 'sqlite:///spider/database\\\\music_2/music_2.sqlite'), ('music_4', 'sqlite:///spider/database\\\\music_4/music_4.sqlite'), ('network_1', 'sqlite:///spider/database\\\\network_1/network_1.sqlite'), ('network_2', 'sqlite:///spider/database\\\\network_2/network_2.sqlite'), ('news_report', 'sqlite:///spider/database\\\\news_report/news_report.sqlite'), ('orchestra', 'sqlite:///spider/database\\\\orchestra/orchestra.sqlite'), ('party_host', 'sqlite:///spider/database\\\\party_host/party_host.sqlite'), ('party_people', 'sqlite:///spider/database\\\\party_people/party_people.sqlite'), ('performance_attendance', 'sqlite:///spider/database\\\\performance_attendance/performance_attendance.sqlite'), ('perpetrator', 'sqlite:///spider/database\\\\perpetrator/perpetrator.sqlite'), ('pets_1', 'sqlite:///spider/database\\\\pets_1/pets_1.sqlite'), ('phone_1', 'sqlite:///spider/database\\\\phone_1/phone_1.sqlite'), ('phone_market', 'sqlite:///spider/database\\\\phone_market/phone_market.sqlite'), ('pilot_record', 'sqlite:///spider/database\\\\pilot_record/pilot_record.sqlite'), ('poker_player', 'sqlite:///spider/database\\\\poker_player/poker_player.sqlite'), ('products_for_hire', 'sqlite:///spider/database\\\\products_for_hire/products_for_hire.sqlite'), ('products_gen_characteristics', 'sqlite:///spider/database\\\\products_gen_characteristics/products_gen_characteristics.sqlite'), ('product_catalog', 'sqlite:///spider/database\\\\product_catalog/product_catalog.sqlite'), ('program_share', 'sqlite:///spider/database\\\\program_share/program_share.sqlite'), ('protein_institute', 'sqlite:///spider/database\\\\protein_institute/protein_institute.sqlite'), ('race_track', 'sqlite:///spider/database\\\\race_track/race_track.sqlite'), ('railway', 'sqlite:///spider/database\\\\railway/railway.sqlite'), ('real_estate_properties', 'sqlite:///spider/database\\\\real_estate_properties/real_estate_properties.sqlite'), ('restaurants', 'sqlite:///spider/database\\\\restaurants/restaurants.sqlite'), ('restaurant_1', 'sqlite:///spider/database\\\\restaurant_1/restaurant_1.sqlite'), ('riding_club', 'sqlite:///spider/database\\\\riding_club/riding_club.sqlite'), ('roller_coaster', 'sqlite:///spider/database\\\\roller_coaster/roller_coaster.sqlite'), ('sakila_1', 'sqlite:///spider/database\\\\sakila_1/sakila_1.sqlite'), ('scholar', 'sqlite:///spider/database\\\\scholar/scholar.sqlite'), ('school_bus', 'sqlite:///spider/database\\\\school_bus/school_bus.sqlite'), ('school_finance', 'sqlite:///spider/database\\\\school_finance/school_finance.sqlite'), ('school_player', 'sqlite:///spider/database\\\\school_player/school_player.sqlite'), ('scientist_1', 'sqlite:///spider/database\\\\scientist_1/scientist_1.sqlite'), ('ship_1', 'sqlite:///spider/database\\\\ship_1/ship_1.sqlite'), ('ship_mission', 'sqlite:///spider/database\\\\ship_mission/ship_mission.sqlite'), ('shop_membership', 'sqlite:///spider/database\\\\shop_membership/shop_membership.sqlite'), ('singer', 'sqlite:///spider/database\\\\singer/singer.sqlite'), ('small_bank_1', 'sqlite:///spider/database\\\\small_bank_1/small_bank_1.sqlite'), ('soccer_1', 'sqlite:///spider/database\\\\soccer_1/soccer_1.sqlite'), ('soccer_2', 'sqlite:///spider/database\\\\soccer_2/soccer_2.sqlite'), ('solvency_ii', 'sqlite:///spider/database\\\\solvency_ii/solvency_ii.sqlite'), ('sports_competition', 'sqlite:///spider/database\\\\sports_competition/sports_competition.sqlite'), ('station_weather', 'sqlite:///spider/database\\\\station_weather/station_weather.sqlite'), ('store_1', 'sqlite:///spider/database\\\\store_1/store_1.sqlite'), ('store_product', 'sqlite:///spider/database\\\\store_product/store_product.sqlite'), ('storm_record', 'sqlite:///spider/database\\\\storm_record/storm_record.sqlite'), ('student_1', 'sqlite:///spider/database\\\\student_1/student_1.sqlite'), ('student_assessment', 'sqlite:///spider/database\\\\student_assessment/student_assessment.sqlite'), ('student_transcripts_tracking', 'sqlite:///spider/database\\\\student_transcripts_tracking/student_transcripts_tracking.sqlite'), ('swimming', 'sqlite:///spider/database\\\\swimming/swimming.sqlite'), ('theme_gallery', 'sqlite:///spider/database\\\\theme_gallery/theme_gallery.sqlite'), ('tracking_grants_for_research', 'sqlite:///spider/database\\\\tracking_grants_for_research/tracking_grants_for_research.sqlite'), ('tracking_orders', 'sqlite:///spider/database\\\\tracking_orders/tracking_orders.sqlite'), ('tracking_share_transactions', 'sqlite:///spider/database\\\\tracking_share_transactions/tracking_share_transactions.sqlite'), ('tracking_software_problems', 'sqlite:///spider/database\\\\tracking_software_problems/tracking_software_problems.sqlite'), ('train_station', 'sqlite:///spider/database\\\\train_station/train_station.sqlite'), ('tvshow', 'sqlite:///spider/database\\\\tvshow/tvshow.sqlite'), ('twitter_1', 'sqlite:///spider/database\\\\twitter_1/twitter_1.sqlite'), ('university_basketball', 'sqlite:///spider/database\\\\university_basketball/university_basketball.sqlite'), ('voter_1', 'sqlite:///spider/database\\\\voter_1/voter_1.sqlite'), ('voter_2', 'sqlite:///spider/database\\\\voter_2/voter_2.sqlite'), ('wedding', 'sqlite:///spider/database\\\\wedding/wedding.sqlite'), ('wine_1', 'sqlite:///spider/database\\\\wine_1/wine_1.sqlite'), ('workshop_paper', 'sqlite:///spider/database\\\\workshop_paper/workshop_paper.sqlite'), ('world_1', 'sqlite:///spider/database\\\\world_1/world_1.sqlite'), ('wrestler', 'sqlite:///spider/database\\\\wrestler/wrestler.sqlite'), ('wta_1', 'sqlite:///spider/database\\\\wta_1/wta_1.sqlite'), ('yelp', 'sqlite:///spider/database\\\\yelp/yelp.sqlite')]\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = 'spider/database'\n",
    "database_paths = [(db, os.path.join(DATASET_PATH, f'{db}/{db}.sqlite')) for db in os.listdir(DATASET_PATH)]\n",
    "database_schema_paths = { db[0]: os.path.join(DATASET_PATH, f'{db[0]}/schema.sql') for db in database_paths }\n",
    "database_con_strings = [(db[0], f'sqlite:///{db[1]}') for db in database_paths]\n",
    "print(\"Found Databases: \", database_con_strings)\n",
    "\n",
    "\n",
    "def crawl_database(con_string, db_id):\n",
    "    engine = create_engine(con_string)\n",
    "\n",
    "    # Reflect the existing database into a new MetaData instance\n",
    "    metadata = MetaData()\n",
    "    metadata.reflect(bind=engine)\n",
    "\n",
    "    # Generate 'CREATE TABLE' statements for all tables\n",
    "    create_table_statements = []\n",
    "    try:\n",
    "        for table in metadata.sorted_tables:\n",
    "            create_statement = str(CreateTable(table).compile(engine)).strip()\n",
    "            create_table_statements.append(create_statement)\n",
    "    except (NoReferencedColumnError, CompileError)  as e:\n",
    "        with open(database_schema_paths[db_id], 'r') as f:\n",
    "            create_table_statements = filter(lambda x: len(x) > 0, map(lambda x: x.strip(), f.readlines()))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Schema not found for {db_id}\")\n",
    "        return ''\n",
    "    return '\\n'.join(create_table_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\3296842037.py:13: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.INTEGER'> with reflected arguments ['11']; using no arguments.\n",
      "  metadata.reflect(bind=engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\3296842037.py:13: SAWarning: WARNING: SQL-parsed foreign key constraint '('Cust_ID', 'customer', 'Cust_ID')' could not be located in PRAGMA foreign_keys for table loan\n",
      "  metadata.reflect(bind=engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\3296842037.py:13: SAWarning: WARNING: SQL-parsed foreign key constraint '('store_id', 'store', 'store_id')' could not be located in PRAGMA foreign_keys for table staff\n",
      "  metadata.reflect(bind=engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\3296842037.py:13: SAWarning: WARNING: SQL-parsed foreign key constraint '('Event_ID', 'Events', 'Event_ID')' could not be located in PRAGMA foreign_keys for table Assets_in_Events\n",
      "  metadata.reflect(bind=engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\3296842037.py:13: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.BIGINT'> with reflected arguments ['20']; using no arguments.\n",
      "  metadata.reflect(bind=engine)\n"
     ]
    }
   ],
   "source": [
    "database_schemas = {db[0]: crawl_database(db[1], db[0]) for db in database_con_strings}\n",
    "with open('database_schemas.json', 'w') as f:\n",
    "    json.dump(database_schemas, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\950617716.py:43: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.INTEGER'> with reflected arguments ['11']; using no arguments.\n",
      "  metadata.reflect(engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\950617716.py:43: SAWarning: WARNING: SQL-parsed foreign key constraint '('Cust_ID', 'customer', 'Cust_ID')' could not be located in PRAGMA foreign_keys for table loan\n",
      "  metadata.reflect(engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\950617716.py:43: SAWarning: WARNING: SQL-parsed foreign key constraint '('store_id', 'store', 'store_id')' could not be located in PRAGMA foreign_keys for table staff\n",
      "  metadata.reflect(engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\950617716.py:43: SAWarning: WARNING: SQL-parsed foreign key constraint '('Event_ID', 'Events', 'Event_ID')' could not be located in PRAGMA foreign_keys for table Assets_in_Events\n",
      "  metadata.reflect(engine)\n",
      "C:\\Users\\saikr\\AppData\\Local\\Temp\\ipykernel_8132\\950617716.py:43: SAWarning: Could not instantiate type <class 'sqlalchemy.sql.sqltypes.BIGINT'> with reflected arguments ['20']; using no arguments.\n",
      "  metadata.reflect(engine)\n"
     ]
    }
   ],
   "source": [
    "databases = {}\n",
    "type_map = []\n",
    "\n",
    "def extract_tables_columns(tables):\n",
    "    def extract_foriegn_keys(foriegn_keys):\n",
    "        result = []\n",
    "        for key in foriegn_keys:\n",
    "            try:\n",
    "                result.append((key.column.table.name, key.column.name))\n",
    "            except Exception as e:\n",
    "                pattern = r\"'(.*?)'\"\n",
    "                matches = re.findall(pattern, str(key))\n",
    "                if not matches:\n",
    "                    matches = str(key).split('.')\n",
    "                if len(matches) == 1:\n",
    "                    result.append(matches[0].split('.'))\n",
    "                elif len(matches) >= 2:\n",
    "                    result.append((matches[0], matches[1]))\n",
    "                else:\n",
    "                    print(f\"Error extracting foreign key: {e}\")\n",
    "        return result\n",
    "\n",
    "# Finding all matches of the pattern in the input string\n",
    "    result = {}\n",
    "    for name, table in tables.items():\n",
    "        for col in table.columns:\n",
    "            col_type = str(col.type).split('(')[0]\n",
    "            try:\n",
    "                col_type_index = type_map.index(col_type)\n",
    "            except ValueError:\n",
    "                type_map.append(col_type)\n",
    "                col_type_index = len(type_map) - 1\n",
    "            if name not in result:\n",
    "                result[name] = []\n",
    "            result[name].append((col.name, col_type_index, int(col.nullable), int(col.primary_key), col.default, extract_foriegn_keys(col.foreign_keys)))\n",
    "    return result\n",
    "\n",
    "\n",
    "for name, db in database_con_strings:\n",
    "    engine = sqlalchemy.create_engine(db)\n",
    "    inspector = sqlalchemy.inspect(engine)\n",
    "    metadata = sqlalchemy.MetaData()\n",
    "    metadata.reflect(engine)\n",
    "    try:\n",
    "        tables = extract_tables_columns(metadata.tables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from {db}: {e}\")\n",
    "    databases[name] = tables\n",
    "\n",
    "with open('databases.json', 'w') as f:\n",
    "    json.dump(databases, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1970 [08:39<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('databases.json') as f:\n",
    "    databases = json.loads(f.read())\n",
    "    \n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    # Remove punctuation\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_dataset_entry(entry):\n",
    "    \"\"\"\n",
    "    This function preprocesses a single dataset entry.\n",
    "    It assumes `database_schemas` is a dictionary with DB schemas accessible by `db_id`.\n",
    "    \"\"\"\n",
    "    processed_text = preprocess_text(entry['question'])\n",
    "    processed_entry = {\n",
    "        'input': f\"translate to SQL: {processed_text} \\n Schema: {json.dumps(databases.get(entry['db_id'], ''))}\",\n",
    "        'target': entry['query'],\n",
    "    }\n",
    "    return processed_entry\n",
    "\n",
    "# Load your dataset\n",
    "with open('spider/train_spider.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "processed_results = list(map(preprocess_dataset_entry, data))\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "dataset_df = pd.DataFrame(processed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(dataset_df)\n",
    "dataset.save_to_disk('spider_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Input Length:  2181\n",
      "Max Target Length:  87\n",
      "Dataset Size:  7000\n"
     ]
    }
   ],
   "source": [
    "max_input_len = max(map(lambda x: len(x['input'].split()), dataset))\n",
    "max_target_len = max(map(lambda x: len(x['target'].split()), dataset))\n",
    "print(\"Max Input Length: \", max_input_len)\n",
    "print(\"Max Target Length: \", max_target_len)\n",
    "print(\"Dataset Size: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT = 't5-small'\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_from_disk\n",
    "tokenizer = AutoTokenizer.from_pretrained(CKPT)\n",
    "model = T5ForConditionalGeneration.from_pretrained(CKPT)\n",
    "\n",
    "dataset = load_from_disk('spider_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.train_test_split(test_size=0.1)\n",
    "train_data = ds['train']\n",
    "val_data = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the examples\n",
    "def convert_to_features(example_batch):\n",
    "    input_encodings = tokenizer.batch_encode_plus(example_batch['input'], pad_to_max_length=True, max_length=2048)\n",
    "    target_encodings = tokenizer.batch_encode_plus(example_batch['target'], pad_to_max_length=True, max_length=128)\n",
    "\n",
    "    encodings = {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids'],\n",
    "        'decoder_attention_mask': target_encodings['attention_mask']\n",
    "    }\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/6300 [00:00<?, ? examples/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\saikr\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(convert_to_features, batched=True, remove_columns=train_data.column_names)\n",
    "test_data = val_data.map(convert_to_features, batched=True, remove_columns=val_data.column_names)\n",
    "\n",
    "columns = ['input_ids', 'attention_mask', 'labels', 'decoder_attention_mask']\n",
    "\n",
    "train_data.set_format(type='torch', columns=columns)\n",
    "test_data.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saikr\\anaconda3\\envs\\torch\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# set training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5-small-finetuned-spider\",\n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=1,\n",
    "    warmup_steps=2,\n",
    "    max_steps=60,\n",
    "    learning_rate=2e-4,\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps = 10,\n",
    "    seed =  42,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    #save_steps=1000,\n",
    "    #eval_steps=1000,\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    #fp16=True,\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saikr\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100%|██████████| 2.37k/2.37k [00:00<?, ?B/s]\n",
      "c:\\Users\\saikr\\anaconda3\\envs\\torch\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saikr\\.cache\\huggingface\\hub\\models--cssupport--t5-small-awesome-text-to-sql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "pytorch_model.bin: 100%|██████████| 242M/242M [00:09<00:00, 26.5MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated SQL query is: SELECT student_id FROM students WHERE NOT student_id IN (SELECT student_id FROM student_course_attendance)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the tokenizer from Hugging Face Transformers library\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('cssupport/t5-small-awesome-text-to-sql')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_sql(input_prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(input_prompt, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=512)\n",
    "    \n",
    "    # Decode the output IDs to a string (SQL query in this case)\n",
    "    generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_sql\n",
    "\n",
    "# Test the function\n",
    "#input_prompt = \"tables:\\n\" + \"CREATE TABLE Catalogs (date_of_latest_revision VARCHAR)\" + \"\\n\" +\"query for: Find the dates on which more than one revisions were made.\"\n",
    "#input_prompt = \"tables:\\n\" + \"CREATE TABLE table_22767 ( \\\"Year\\\" real, \\\"World\\\" real, \\\"Asia\\\" text, \\\"Africa\\\" text, \\\"Europe\\\" text, \\\"Latin America/Caribbean\\\" text, \\\"Northern America\\\" text, \\\"Oceania\\\" text )\" + \"\\n\" +\"query for:what will the population of Asia be when Latin America/Caribbean is 783 (7.5%)?.\"\n",
    "#input_prompt = \"tables:\\n\" + \"CREATE TABLE procedures ( subject_id text, hadm_id text, icd9_code text, short_title text, long_title text ) CREATE TABLE diagnoses ( subject_id text, hadm_id text, icd9_code text, short_title text, long_title text ) CREATE TABLE lab ( subject_id text, hadm_id text, itemid text, charttime text, flag text, value_unit text, label text, fluid text ) CREATE TABLE demographic ( subject_id text, hadm_id text, name text, marital_status text, age text, dob text, gender text, language text, religion text, admission_type text, days_stay text, insurance text, ethnicity text, expire_flag text, admission_location text, discharge_location text, diagnosis text, dod text, dob_year text, dod_year text, admittime text, dischtime text, admityear text ) CREATE TABLE prescriptions ( subject_id text, hadm_id text, icustay_id text, drug_type text, drug text, formulary_drug_cd text, route text, drug_dose text )\" + \"\\n\" +\"query for:\" + \"what is the total number of patients who were diagnosed with icd9 code 2254?\"\n",
    "input_prompt = \"tables:\\n\" + \"CREATE TABLE student_course_attendance (student_id VARCHAR); CREATE TABLE students (student_id VARCHAR)\" + \"\\n\" + \"query for:\" + \"List the id of students who never attends courses?\"\n",
    "\n",
    "generated_sql = generate_sql(input_prompt)\n",
    "\n",
    "print(f\"The generated SQL query is: {generated_sql}\")\n",
    "#OUTPUT: The generated SQL query is: SELECT student_id FROM students WHERE NOT student_id IN (SELECT student_id FROM student_course_attendance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5-small-finetuned-spider\\\\tokenizer_config.json',\n",
       " 't5-small-finetuned-spider\\\\special_tokens_map.json',\n",
       " 't5-small-finetuned-spider\\\\spiece.model',\n",
       " 't5-small-finetuned-spider\\\\added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('t5-small-finetuned-spider')\n",
    "tokenizer.save_pretrained('t5-small-finetuned-spider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
